{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4327e8bd-ca94-4d90-a9e8-4b3ceb83d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss, roc_auc_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import joblib\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f52f46-2dad-4128-b198-2b44dad31dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda1a474-c881-4738-8d7c-d2a4c85d6e04",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\data\\\\6th sem\\\\Big data analytics\\\\theory project\\\\archive\\\\Books.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m users_path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m6th sem\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mBig data analytics\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtheory project\u001b[39m\u001b[33m\\\u001b[39m\u001b[33marchive\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the data into Pandas DataFrames\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m books_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbooks_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatin1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use 'latin1' if there's special encoding\u001b[39;00m\n\u001b[32m      8\u001b[39m ratings_df = pd.read_csv(ratings_path, encoding=\u001b[33m'\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m users_df = pd.read_csv(users_path, encoding=\u001b[33m'\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'D:\\\\data\\\\6th sem\\\\Big data analytics\\\\theory project\\\\archive\\\\Books.csv'"
     ]
    }
   ],
   "source": [
    "# Define the file paths\n",
    "books_path = r\"D:\\data\\6th sem\\Big data analytics\\theory project\\archive\\data\\Books.csv\"\n",
    "ratings_path = r\"D:\\data\\6th sem\\Big data analytics\\theory project\\archive\\data\\Ratings.csv\"\n",
    "users_path = r\"D:\\data\\6th sem\\Big data analytics\\theory project\\archive\\Users.csv\"\n",
    "\n",
    "# Load the data into Pandas DataFrames\n",
    "books_df = pd.read_csv(books_path, encoding='latin1')  # Use 'latin1' if there's special encoding\n",
    "ratings_df = pd.read_csv(ratings_path, encoding='latin1')\n",
    "users_df = pd.read_csv(users_path, encoding='latin1')\n",
    "\n",
    "# Display the first few rows of each DataFrame to verify loading\n",
    "print(\"Books Data:\")\n",
    "print(books_df.head())\n",
    "\n",
    "print(\"\\nRatings Data:\")\n",
    "print(ratings_df.head())\n",
    "\n",
    "print(\"\\nUsers Data:\")\n",
    "print(users_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f37957-fd18-4692-af4a-f1f689c54929",
   "metadata": {},
   "source": [
    "Step 1: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c6979-84a0-4c0a-8537-409cf58d1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Books Missing Values:\\n\", books_df.isnull().sum())\n",
    "print(\"\\nRatings Missing Values:\\n\", ratings_df.isnull().sum())\n",
    "print(\"\\nUsers Missing Values:\\n\", users_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89b2bc-20a6-47dd-98d7-301a59892d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Year-Of-Publication to numeric, handling invalid entries as NaN\n",
    "books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce')\n",
    "\n",
    "# Fill missing values in Year-Of-Publication with the median year\n",
    "median_year = books_df['Year-Of-Publication'].median()\n",
    "books_df['Year-Of-Publication'] = books_df['Year-Of-Publication'].fillna(median_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949d8dd-2a1f-4a74-a9b5-f823302b214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median age\n",
    "median_age = users_df['Age'].median()\n",
    "\n",
    "# Fill missing values in the Age column\n",
    "users_df['Age'] = users_df['Age'].fillna(median_age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbce53e-2721-48c7-befb-1fc5cb989980",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fdf827-68ff-443d-90e4-f8b78c9d88df",
   "metadata": {},
   "source": [
    "Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e6bb0-bfec-4452-98b0-77b90c88a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "books_df['Decade'] = (books_df['Year-Of-Publication'] // 10) * 10\n",
    "location_split = users_df['Location'].str.split(',', expand=True)\n",
    "users_df['Country'] = location_split[2].str.strip().fillna('Unknown')\n",
    "users_df.drop(columns=['Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861d654-03f3-4a34-b48d-b969133546a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_avg_rating = ratings_df.groupby('ISBN')['Book-Rating'].mean().reset_index(name='Avg_Book_Rating')\n",
    "user_avg_rating = ratings_df.groupby('User-ID')['Book-Rating'].mean().reset_index(name='Avg_User_Rating')\n",
    "\n",
    "ratings_with_details = ratings_df.merge(books_df[['ISBN', 'Book-Title', 'Book-Author', 'Decade']], on='ISBN', how='inner')\\\n",
    "                                .merge(users_df[['User-ID', 'Age', 'Country']], on='User-ID', how='inner')\\\n",
    "                                .merge(book_avg_rating, on='ISBN', how='inner')\\\n",
    "                                .merge(user_avg_rating, on='User-ID', how='inner')\n",
    "\n",
    "ratings_with_details['High_Rating'] = (ratings_with_details['Book-Rating'] >= 7).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ec97d-c639-411e-98de-e9eab90e9423",
   "metadata": {},
   "source": [
    "Step 3: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2681854-18b9-4bad-92ee-0eab716a3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_avg_rating = ratings_df.groupby('ISBN')['Book-Rating'].mean().reset_index(name='Avg_Book_Rating')\n",
    "user_avg_rating = ratings_df.groupby('User-ID')['Book-Rating'].mean().reset_index(name='Avg_User_Rating')\n",
    "\n",
    "ratings_with_details = ratings_df.merge(books_df[['ISBN', 'Book-Title', 'Book-Author', 'Decade']], on='ISBN', how='inner')\\\n",
    "                                .merge(users_df[['User-ID', 'Age', 'Country']], on='User-ID', how='inner')\\\n",
    "                                .merge(book_avg_rating, on='ISBN', how='inner')\\\n",
    "                                .merge(user_avg_rating, on='User-ID', how='inner')\n",
    "\n",
    "ratings_with_details['High_Rating'] = (ratings_with_details['Book-Rating'] >= 7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417a305-8995-4f89-a953-7572cea0c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "ratings_with_details['Country_Encoded'] = label_encoder.fit_transform(ratings_with_details['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36e473-c749-48a1-90f8-3285c0e7ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP: TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "books_df['Text'] = books_df['Book-Title'].fillna('') + ' ' + books_df['Book-Author'].fillna('')\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = tfidf.fit_transform(books_df['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83328f1-00ce-4a90-aab0-616e7a4b419f",
   "metadata": {},
   "source": [
    "Step 4: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5958b04-919e-450e-9159-889b67bdd288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "ratings_with_details.to_csv('processed_ratings_with_details.csv', index=False)\n",
    "np.save('tfidf_matrix.npy', tfidf_matrix.toarray())\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f3edf-00b5-49bc-9255-35e58509bdbe",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be23ca-9d08-421f-bc2d-cb07558e1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "feature_columns = ['Age', 'Avg_Book_Rating', 'Avg_User_Rating', 'Decade', 'Country_Encoded']\n",
    "X = ratings_with_details[feature_columns]\n",
    "y = ratings_with_details['High_Rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d954ee-1438-4315-9ed1-1ea3b75e1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow setup\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment('book_recommendation')\n",
    "\n",
    "def train_and_log_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        logloss = log_loss(y_test, y_proba) if y_proba is not None else None\n",
    "        roc_auc = roc_auc_score(y_test, y_proba[:, 1]) if y_proba is not None else None\n",
    "        mlflow.log_metric('accuracy', accuracy)\n",
    "        if logloss is not None:\n",
    "            mlflow.log_metric('log_loss', logloss)\n",
    "        if roc_auc is not None:\n",
    "            mlflow.log_metric('roc_auc', roc_auc)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        mlflow.log_dict(report, 'classification_report.json')\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "        joblib.dump(model, f'{model_name}.joblib')\n",
    "        return accuracy, logloss, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6241c47-39d4-4fd9-a803-7aec3135ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "rf_model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42, n_jobs=-1)\n",
    "xgb_model = XGBClassifier(n_estimators=50, max_depth=3, subsample=0.8, n_jobs=-1, eval_metric='logloss', random_state=42)\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1e170-a0a8-4bbc-bcc0-6939ac98fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "results = []\n",
    "results.append(train_and_log_model(rf_model, 'RandomForest', X_train, X_test, y_train, y_test))\n",
    "results.append(train_and_log_model(xgb_model, 'XGBoost', X_train, X_test, y_train, y_test))\n",
    "results.append(train_and_log_model(lr_model, 'LogisticRegression', X_train, X_test, y_train, y_test))\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(results, columns=['Accuracy', 'Log-Loss', 'ROC-AUC'],\n",
    "                          index=['RandomForest', 'XGBoost', 'LogisticRegression'])\n",
    "results_df.to_csv('model_comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06750da9-9e95-4c28-8c23-9ac360e854c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation function\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_book(user_id, book_title, model, tfidf, tfidf_matrix, books_df, ratings_with_details):\n",
    "    # Collaborative filtering: Predict high rating probability\n",
    "    user_data = ratings_with_details[ratings_with_details['User-ID'] == user_id][feature_columns].mean()\n",
    "    if user_data.empty:\n",
    "        user_data = ratings_with_details[feature_columns].mean()\n",
    "    pred_proba = model.predict_proba([user_data])[0][1] if hasattr(model, 'predict_proba') else 0.5\n",
    "    \n",
    "    # Content-based filtering: Find similar books\n",
    "    book_idx = books_df[books_df['Book-Title'].str.contains(book_title, case=False, na=False)].index\n",
    "    if not book_idx.empty:\n",
    "        book_idx = book_idx[0]\n",
    "        similarities = cosine_similarity(tfidf_matrix[book_idx], tfidf_matrix).flatten()\n",
    "        similar_indices = similarities.argsort()[-10:][::-1]\n",
    "        similar_books = books_df.iloc[similar_indices]\n",
    "        # Filter by popularity (high average rating)\n",
    "        similar_books = similar_books.merge(book_avg_rating, on='ISBN', how='left')\n",
    "        similar_books = similar_books[similar_books['Avg_Book_Rating'] >= 7]\n",
    "        if not similar_books.empty:\n",
    "            return similar_books.iloc[0][['Book-Title', 'Book-Author', 'Avg_Book_Rating']]\n",
    "    \n",
    "    # Default to most popular book if no match\n",
    "    popular_book = ratings_with_details.merge(books_df[['ISBN', 'Book-Title', 'Book-Author']], on='ISBN')\\\n",
    "                                      .groupby(['ISBN', 'Book-Title', 'Book-Author'])['Book-Rating'].mean()\\\n",
    "                                      .reset_index().sort_values('Book-Rating', ascending=False).iloc[0]\n",
    "    return popular_book[['Book-Title', 'Book-Author', 'Book-Rating']]\n",
    "\n",
    "# Example recommendation\n",
    "recommended_book = recommend_book(user_id=276725, book_title='Clara Callan', model=xgb_model,\n",
    "                                 tfidf=tfidf, tfidf_matrix=tfidf_matrix, books_df=books_df,\n",
    "                                 ratings_with_details=ratings_with_details)\n",
    "print('Recommended Book:', recommended_book['Book-Title'], 'by', recommended_book['Book-Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804abd7-98cf-4e7a-b943-7672eb8e174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "\n",
    "def upload_to_s3(file_path, bucket_name, s3_path):\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.upload_file(file_path, bucket_name, s3_path)\n",
    "        print(f'Uploaded {file_path} to s3://{bucket_name}/{s3_path}')\n",
    "    except NoCredentialsError:\n",
    "        print(f'Failed to upload {file_path}: AWS credentials not configured.')\n",
    "        print('Please configure AWS credentials using \"aws configure\" or set environment variables:')\n",
    "        print('  export AWS_ACCESS_KEY_ID=\"your_access_key\"')\n",
    "        print('  export AWS_SECRET_ACCESS_KEY=\"your_secret_key\"')\n",
    "        print('  export AWS_DEFAULT_REGION=\"eu-north-1\"')\n",
    "    except ClientError as e:\n",
    "        print(f'Failed to upload {file_path}: {e}')\n",
    "\n",
    "bucket_name = 'book-recommendation-bucket123'\n",
    "upload_to_s3('processed_ratings_with_details.csv', bucket_name, 'data/processed_ratings_with_details.csv')\n",
    "upload_to_s3('RandomForest.joblib', bucket_name, 'models/RandomForest.joblib')\n",
    "upload_to_s3('XGBoost.joblib', bucket_name, 'models/XGBoost.joblib')\n",
    "upload_to_s3('LogisticRegression.joblib', bucket_name, 'models/LogisticRegression.joblib')\n",
    "upload_to_s3('model_comparison.csv', bucket_name, 'results/model_comparison.csv')\n",
    "upload_to_s3('tfidf_vectorizer.joblib', bucket_name, 'models/tfidf_vectorizer.joblib')\n",
    "upload_to_s3('tfidf_matrix.npy', bucket_name, 'models/tfidf_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd77eb-1e8e-444f-a9d6-0748860a536e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
