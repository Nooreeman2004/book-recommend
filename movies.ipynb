{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4327e8bd-ca94-4d90-a9e8-4b3ceb83d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss, roc_auc_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import joblib\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f52f46-2dad-4128-b198-2b44dad31dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dda1a474-c881-4738-8d7c-d2a4c85d6e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noor\\AppData\\Local\\Temp\\ipykernel_3900\\696444792.py:7: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books_df = pd.read_csv(books_path, encoding='latin1')  # Use 'latin1' if there's special encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books Data:\n",
      "         ISBN                                         Book-Title  \\\n",
      "0  0195153448                                Classical Mythology   \n",
      "1  0002005018                                       Clara Callan   \n",
      "2  0060973129                               Decision in Normandy   \n",
      "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
      "4  0393045218                             The Mummies of Urumchi   \n",
      "\n",
      "            Book-Author Year-Of-Publication                   Publisher  \\\n",
      "0    Mark P. O. Morford                2002     Oxford University Press   \n",
      "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
      "2          Carlo D'Este                1991             HarperPerennial   \n",
      "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
      "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
      "\n",
      "                                         Image-URL-S  \\\n",
      "0  http://images.amazon.com/images/P/0195153448.0...   \n",
      "1  http://images.amazon.com/images/P/0002005018.0...   \n",
      "2  http://images.amazon.com/images/P/0060973129.0...   \n",
      "3  http://images.amazon.com/images/P/0374157065.0...   \n",
      "4  http://images.amazon.com/images/P/0393045218.0...   \n",
      "\n",
      "                                         Image-URL-M  \\\n",
      "0  http://images.amazon.com/images/P/0195153448.0...   \n",
      "1  http://images.amazon.com/images/P/0002005018.0...   \n",
      "2  http://images.amazon.com/images/P/0060973129.0...   \n",
      "3  http://images.amazon.com/images/P/0374157065.0...   \n",
      "4  http://images.amazon.com/images/P/0393045218.0...   \n",
      "\n",
      "                                         Image-URL-L  \n",
      "0  http://images.amazon.com/images/P/0195153448.0...  \n",
      "1  http://images.amazon.com/images/P/0002005018.0...  \n",
      "2  http://images.amazon.com/images/P/0060973129.0...  \n",
      "3  http://images.amazon.com/images/P/0374157065.0...  \n",
      "4  http://images.amazon.com/images/P/0393045218.0...  \n",
      "\n",
      "Ratings Data:\n",
      "   User-ID        ISBN  Book-Rating\n",
      "0   276725  034545104X            0\n",
      "1   276726  0155061224            5\n",
      "2   276727  0446520802            0\n",
      "3   276729  052165615X            3\n",
      "4   276729  0521795028            6\n",
      "\n",
      "Users Data:\n",
      "   User-ID                            Location   Age\n",
      "0        1                  nyc, new york, usa   NaN\n",
      "1        2           stockton, california, usa  18.0\n",
      "2        3     moscow, yukon territory, russia   NaN\n",
      "3        4           porto, v.n.gaia, portugal  17.0\n",
      "4        5  farnborough, hants, united kingdom   NaN\n"
     ]
    }
   ],
   "source": [
    "# Define the file paths\n",
    "books_path = r\"D:\\data\\6th sem\\Big data analytics\\theory project\\archive\\data\\Books.csv\"\n",
    "ratings_path = r\"D:\\data\\6th sem\\Big data analytics\\theory project\\archive\\data\\Ratings.csv\"\n",
    "users_path = r\"D:\\data\\6th sem\\Big data analytics\\theory project\\archive\\Users.csv\"\n",
    "\n",
    "# Load the data into Pandas DataFrames\n",
    "books_df = pd.read_csv(books_path, encoding='latin1')  # Use 'latin1' if there's special encoding\n",
    "ratings_df = pd.read_csv(ratings_path, encoding='latin1')\n",
    "users_df = pd.read_csv(users_path, encoding='latin1')\n",
    "\n",
    "# Display the first few rows of each DataFrame to verify loading\n",
    "print(\"Books Data:\")\n",
    "print(books_df.head())\n",
    "\n",
    "print(\"\\nRatings Data:\")\n",
    "print(ratings_df.head())\n",
    "\n",
    "print(\"\\nUsers Data:\")\n",
    "print(users_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f37957-fd18-4692-af4a-f1f689c54929",
   "metadata": {},
   "source": [
    "Step 1: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "823c6979-84a0-4c0a-8537-409cf58d1bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books Missing Values:\n",
      " ISBN                   0\n",
      "Book-Title             0\n",
      "Book-Author            2\n",
      "Year-Of-Publication    0\n",
      "Publisher              2\n",
      "Image-URL-S            0\n",
      "Image-URL-M            0\n",
      "Image-URL-L            3\n",
      "dtype: int64\n",
      "\n",
      "Ratings Missing Values:\n",
      " User-ID        0\n",
      "ISBN           0\n",
      "Book-Rating    0\n",
      "dtype: int64\n",
      "\n",
      "Users Missing Values:\n",
      " User-ID          0\n",
      "Location         0\n",
      "Age         110762\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Books Missing Values:\\n\", books_df.isnull().sum())\n",
    "print(\"\\nRatings Missing Values:\\n\", ratings_df.isnull().sum())\n",
    "print(\"\\nUsers Missing Values:\\n\", users_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db89b2bc-20a6-47dd-98d7-301a59892d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Year-Of-Publication to numeric, handling invalid entries as NaN\n",
    "books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce')\n",
    "\n",
    "# Fill missing values in Year-Of-Publication with the median year\n",
    "median_year = books_df['Year-Of-Publication'].median()\n",
    "books_df['Year-Of-Publication'] = books_df['Year-Of-Publication'].fillna(median_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b949d8dd-2a1f-4a74-a9b5-f823302b214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median age\n",
    "median_age = users_df['Age'].median()\n",
    "\n",
    "# Fill missing values in the Age column\n",
    "users_df['Age'] = users_df['Age'].fillna(median_age)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fbce53e-2721-48c7-befb-1fc5cb989980",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fdf827-68ff-443d-90e4-f8b78c9d88df",
   "metadata": {},
   "source": [
    "Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601e6bb0-bfec-4452-98b0-77b90c88a7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "books_df['Decade'] = (books_df['Year-Of-Publication'] // 10) * 10\n",
    "location_split = users_df['Location'].str.split(',', expand=True)\n",
    "users_df['Country'] = location_split[2].str.strip().fillna('Unknown')\n",
    "users_df.drop(columns=['Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f861d654-03f3-4a34-b48d-b969133546a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_avg_rating = ratings_df.groupby('ISBN')['Book-Rating'].mean().reset_index(name='Avg_Book_Rating')\n",
    "user_avg_rating = ratings_df.groupby('User-ID')['Book-Rating'].mean().reset_index(name='Avg_User_Rating')\n",
    "\n",
    "ratings_with_details = ratings_df.merge(books_df[['ISBN', 'Book-Title', 'Book-Author', 'Decade']], on='ISBN', how='inner')\\\n",
    "                                .merge(users_df[['User-ID', 'Age', 'Country']], on='User-ID', how='inner')\\\n",
    "                                .merge(book_avg_rating, on='ISBN', how='inner')\\\n",
    "                                .merge(user_avg_rating, on='User-ID', how='inner')\n",
    "\n",
    "ratings_with_details['High_Rating'] = (ratings_with_details['Book-Rating'] >= 7).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ec97d-c639-411e-98de-e9eab90e9423",
   "metadata": {},
   "source": [
    "Step 3: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2681854-18b9-4bad-92ee-0eab716a3297",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_avg_rating = ratings_df.groupby('ISBN')['Book-Rating'].mean().reset_index(name='Avg_Book_Rating')\n",
    "user_avg_rating = ratings_df.groupby('User-ID')['Book-Rating'].mean().reset_index(name='Avg_User_Rating')\n",
    "\n",
    "ratings_with_details = ratings_df.merge(books_df[['ISBN', 'Book-Title', 'Book-Author', 'Decade']], on='ISBN', how='inner')\\\n",
    "                                .merge(users_df[['User-ID', 'Age', 'Country']], on='User-ID', how='inner')\\\n",
    "                                .merge(book_avg_rating, on='ISBN', how='inner')\\\n",
    "                                .merge(user_avg_rating, on='User-ID', how='inner')\n",
    "\n",
    "ratings_with_details['High_Rating'] = (ratings_with_details['Book-Rating'] >= 7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9417a305-8995-4f89-a953-7572cea0c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "ratings_with_details['Country_Encoded'] = label_encoder.fit_transform(ratings_with_details['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e36e473-c749-48a1-90f8-3285c0e7ae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books DataFrame loaded successfully!\n",
      "Book titles column processed for missing values.\n",
      "TF-IDF vectorization completed!\n",
      "TF-IDF vectorizer saved successfully!\n",
      "TF-IDF matrix saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Load books_df\n",
    "books_path = \"D:/data/6th sem/Big data analytics/theory project/archive/data/Books.csv\"\n",
    "try:\n",
    "    books_df = pd.read_csv(books_path, encoding='latin1', dtype={'Year-Of-Publication': str})\n",
    "    print(\"Books DataFrame loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {books_path} not found.\")\n",
    "    exit()\n",
    "\n",
    "# Handle missing values in Book-Title\n",
    "data = books_df['Book-Title'].fillna('')\n",
    "print(\"Book titles column processed for missing values.\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "try:\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Reduced for efficiency\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "    print(\"TF-IDF vectorization completed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during TF-IDF processing: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Save Outputs\n",
    "output_dir = \"D:/data/6th sem/Big data analytics/theory project/archive/backend/models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    joblib.dump(tfidf_vectorizer, f\"{output_dir}/tfidf_vectorizer.joblib\")\n",
    "    print(\"TF-IDF vectorizer saved successfully!\")\n",
    "    save_npz(f\"{output_dir}/tfidf_matrix.npz\", tfidf_matrix)\n",
    "    print(\"TF-IDF matrix saved successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while saving files: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83328f1-00ce-4a90-aab0-616e7a4b419f",
   "metadata": {},
   "source": [
    "Step 4: Save Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f3edf-00b5-49bc-9255-35e58509bdbe",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5be23ca-9d08-421f-bc2d-cb07558e1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "feature_columns = ['Age', 'Avg_Book_Rating', 'Avg_User_Rating', 'Decade', 'Country_Encoded']\n",
    "X = ratings_with_details[feature_columns]\n",
    "y = ratings_with_details['High_Rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79d954ee-1438-4315-9ed1-1ea3b75e1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow setup\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_experiment('book_recommendation')\n",
    "\n",
    "def train_and_log_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        logloss = log_loss(y_test, y_proba) if y_proba is not None else None\n",
    "        roc_auc = roc_auc_score(y_test, y_proba[:, 1]) if y_proba is not None else None\n",
    "        mlflow.log_metric('accuracy', accuracy)\n",
    "        if logloss is not None:\n",
    "            mlflow.log_metric('log_loss', logloss)\n",
    "        if roc_auc is not None:\n",
    "            mlflow.log_metric('roc_auc', roc_auc)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        mlflow.log_dict(report, 'classification_report.json')\n",
    "        mlflow.sklearn.log_model(model, model_name)\n",
    "        joblib.dump(model, f'{model_name}.joblib')\n",
    "        return accuracy, logloss, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6241c47-39d4-4fd9-a803-7aec3135ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42, n_jobs=-1)\n",
    "xgb_model = XGBClassifier(n_estimators=50, max_depth=3, subsample=0.8, n_jobs=-1, eval_metric='logloss', random_state=42)\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5c1e170-a0a8-4bbc-bcc0-6939ac98fd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/15 17:30:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/06/15 17:30:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/06/15 17:30:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "results = []\n",
    "results.append(train_and_log_model(rf_model, 'RandomForest', X_train, X_test, y_train, y_test))\n",
    "results.append(train_and_log_model(xgb_model, 'XGBoost', X_train, X_test, y_train, y_test))\n",
    "results.append(train_and_log_model(lr_model, 'LogisticRegression', X_train, X_test, y_train, y_test))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Accuracy', 'Log-Loss', 'ROC-AUC'],\n",
    "                          index=['RandomForest', 'XGBoost', 'LogisticRegression'])\n",
    "results_df.to_csv('D:/data/6th sem/Big data analytics/theory project/archive/data/model_comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06750da9-9e95-4c28-8c23-9ac360e854c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book: Haiku : This Other World by Richard Wright\n"
     ]
    }
   ],
   "source": [
    "# Recommendation function\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_book(user_id, book_title, model, tfidf, tfidf_matrix, books_df, ratings_with_details):\n",
    "    # Collaborative filtering: Predict high rating probability\n",
    "    user_data = ratings_with_details[ratings_with_details['User-ID'] == user_id][feature_columns].mean()\n",
    "    if user_data.empty:\n",
    "        user_data = ratings_with_details[feature_columns].mean()\n",
    "    pred_proba = model.predict_proba([user_data])[0][1] if hasattr(model, 'predict_proba') else 0.5\n",
    "    \n",
    "    # Content-based filtering: Find similar books\n",
    "    book_idx = books_df[books_df['Book-Title'].str.contains(book_title, case=False, na=False)].index\n",
    "    if not book_idx.empty:\n",
    "        book_idx = book_idx[0]\n",
    "        similarities = cosine_similarity(tfidf_matrix[book_idx], tfidf_matrix).flatten()\n",
    "        similar_indices = similarities.argsort()[-10:][::-1]\n",
    "        similar_books = books_df.iloc[similar_indices]\n",
    "        # Filter by popularity (high average rating)\n",
    "        similar_books = similar_books.merge(book_avg_rating, on='ISBN', how='left')\n",
    "        similar_books = similar_books[similar_books['Avg_Book_Rating'] >= 7]\n",
    "        if not similar_books.empty:\n",
    "            return similar_books.iloc[0][['Book-Title', 'Book-Author', 'Avg_Book_Rating']]\n",
    "    \n",
    "    # Default to most popular book if no match\n",
    "    popular_book = ratings_with_details.merge(books_df[['ISBN', 'Book-Title', 'Book-Author']], on='ISBN')\\\n",
    "                                      .groupby(['ISBN', 'Book-Title', 'Book-Author'])['Book-Rating'].mean()\\\n",
    "                                      .reset_index().sort_values('Book-Rating', ascending=False).iloc[0]\n",
    "    return popular_book[['Book-Title', 'Book-Author', 'Book-Rating']]\n",
    "\n",
    "# Example recommendation\n",
    "recommended_book = recommend_book(user_id=276725, book_title='Clara Callan', model=xgb_model,\n",
    "                                 tfidf=tfidf, tfidf_matrix=tfidf_matrix, books_df=books_df,\n",
    "                                 ratings_with_details=ratings_with_details)\n",
    "print('Recommended Book:', recommended_book['Book-Title'], 'by', recommended_book['Book-Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6804abd7-98cf-4e7a-b943-7672eb8e174a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFailed to upload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     18\u001b[39m bucket_name = \u001b[33m'\u001b[39m\u001b[33mbook-recommendation-bucket123\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mupload_to_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprocessed_ratings_with_details.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/processed_ratings_with_details.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m upload_to_s3(\u001b[33m'\u001b[39m\u001b[33mRandomForest.joblib\u001b[39m\u001b[33m'\u001b[39m, bucket_name, \u001b[33m'\u001b[39m\u001b[33mmodels/RandomForest.joblib\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m upload_to_s3(\u001b[33m'\u001b[39m\u001b[33mXGBoost.joblib\u001b[39m\u001b[33m'\u001b[39m, bucket_name, \u001b[33m'\u001b[39m\u001b[33mmodels/XGBoost.joblib\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mupload_to_s3\u001b[39m\u001b[34m(file_path, bucket_name, s3_path)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      6\u001b[39m     s3 = boto3.client(\u001b[33m'\u001b[39m\u001b[33ms3\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43ms3\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUploaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to s3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms3_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NoCredentialsError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botocore\\context.py:123\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    122\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\boto3\\s3\\inject.py:175\u001b[39m, in \u001b[36mupload_file\u001b[39m\u001b[34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Upload a file to an S3 object.\u001b[39;00m\n\u001b[32m    141\u001b[39m \n\u001b[32m    142\u001b[39m \u001b[33;03mUsage::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m \u001b[33;03m    transfer.\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\boto3\\s3\\transfer.py:372\u001b[39m, in \u001b[36mS3Transfer.upload_file\u001b[39m\u001b[34m(self, filename, bucket, key, callback, extra_args)\u001b[39m\n\u001b[32m    368\u001b[39m future = \u001b[38;5;28mself\u001b[39m._manager.upload(\n\u001b[32m    369\u001b[39m     filename, bucket, key, extra_args, subscribers\n\u001b[32m    370\u001b[39m )\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m# client error.\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\futures.py:114\u001b[39m, in \u001b[36mTransferFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.cancel()\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\futures.py:111\u001b[39m, in \u001b[36mTransferFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    108\u001b[39m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[32m    109\u001b[39m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[32m    110\u001b[39m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_coordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    113\u001b[39m         \u001b[38;5;28mself\u001b[39m.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\s3transfer\\futures.py:282\u001b[39m, in \u001b[36mTransferCoordinator.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Waits until TransferFuture is done and returns the result\u001b[39;00m\n\u001b[32m    273\u001b[39m \n\u001b[32m    274\u001b[39m \u001b[33;03mIf the TransferFuture succeeded, it will return the result. If the\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03mTransferFuture failed, it will raise the exception associated to the\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03mfailure.\u001b[39;00m\n\u001b[32m    277\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# Doing a wait() with no timeout cannot be interrupted in python2 but\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# can be interrupted in python3 so we just wait with the largest\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# possible value integer value, which is on the scale of billions of\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# years...\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_done_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMAXINT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "\n",
    "def upload_to_s3(file_path, bucket_name, s3_path):\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.upload_file(file_path, bucket_name, s3_path)\n",
    "        print(f'Uploaded {file_path} to s3://{bucket_name}/{s3_path}')\n",
    "    except NoCredentialsError:\n",
    "        print(f'Failed to upload {file_path}: AWS credentials not configured.')\n",
    "        print('Please configure AWS credentials using \"aws configure\" or set environment variables:')\n",
    "        print('  export AWS_ACCESS_KEY_ID=\"your_access_key\"')\n",
    "        print('  export AWS_SECRET_ACCESS_KEY=\"your_secret_key\"')\n",
    "        print('  export AWS_DEFAULT_REGION=\"eu-north-1\"')\n",
    "    except ClientError as e:\n",
    "        print(f'Failed to upload {file_path}: {e}')\n",
    "\n",
    "bucket_name = 'book-recommendation-bucket123'\n",
    "upload_to_s3('processed_ratings_with_details.csv', bucket_name, 'data/processed_ratings_with_details.csv')\n",
    "upload_to_s3('RandomForest.joblib', bucket_name, 'models/RandomForest.joblib')\n",
    "upload_to_s3('XGBoost.joblib', bucket_name, 'models/XGBoost.joblib')\n",
    "upload_to_s3('LogisticRegression.joblib', bucket_name, 'models/LogisticRegression.joblib')\n",
    "upload_to_s3('model_comparison.csv', bucket_name, 'results/model_comparison.csv')\n",
    "upload_to_s3('tfidf_vectorizer.joblib', bucket_name, 'models/tfidf_vectorizer.joblib')\n",
    "upload_to_s3('tfidf_matrix.npy', bucket_name, 'models/tfidf_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd77eb-1e8e-444f-a9d6-0748860a536e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbeb951c-aa31-4e3a-ad7f-13e8aa015b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at D:/data/6th sem/Big data analytics/theory project/archive/backend/models\\XGBoost.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Define the path\n",
    "model_dir = \"D:/data/6th sem/Big data analytics/theory project/archive/backend/models\"\n",
    "model_path = os.path.join(model_dir, \"XGBoost.joblib\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Example model training\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Model saved at {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5252026-f51a-4e54-897c-83328012839b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(824908, 5)\n",
      "(824908,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)  # Input features shape\n",
    "print(y_train.shape)  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab032e5-46d4-4e9d-a2ff-183d02f6b65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
